{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook is essentially to understand how forecasting in financial instruments works for given data\n",
        "\n",
        "We will go through a lot of theory and mathematics behind this to come up with a thorough understanding of this"
      ],
      "metadata": {
        "id": "I8Hgq9hwWwfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters of Forecasting Accuracy\n",
        "**Hit Rate**\n",
        "\n",
        "This essentially means the percentage of correct predictions : just jargon for accuracy ; no big deal\n",
        "\n",
        "**Confusion Matrix**\n",
        "\n",
        "In case of financial instruments, the price may move up or down and our predictions for the same may be correct or incorrect\n",
        "\n",
        "It is obvious then that there arise 4 possible cases in the testing phase : let us generalize the *up* and *down* to positives and negatives respectively\n",
        "\n",
        "Therefore we have true positives, true negatives, false positives, and false negatives : the last two make up Type 1 Error and Type 2 Error in jargon respectively\n",
        "\n",
        "A 2 x 2 matrix represents these values\n",
        "\n",
        "The first row contains the true and false positive rates , second row the false and true negative rates"
      ],
      "metadata": {
        "id": "rpnsGXxJXSq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choosing Appropriate Factors\n",
        "\n",
        "Just fitting in some algorithm does not help : we need to find the appropriate factors that help us identify key trends and achieve better results\n",
        "\n",
        "**Lagged Price Factors**\n",
        "\n",
        "The prior historical values of the time series are extremely useful in forecasting the time series\n",
        "\n",
        "A set of p factors can be obtained by creating p lags of the time series close price\n",
        "\n",
        "We may simply consider a current day x ; our factors would be the historical daily values of the financial instrument at time periods x-1, x-2, and so on till x- p\n",
        "\n",
        "**Traded Volume**\n",
        "\n",
        "Traded volume indicates momentum or interest within the financial instrument ; if the volume is low, there is less interest and therefore not much earning potential\n",
        "\n",
        "Opposite goes for a high volume\n",
        "\n",
        "\n",
        "Combining these two, we can create a p+1 dimensional feature vector for each day of the time series , which incorporates the p time lags and the traded volume on that day\n",
        "\n",
        "We therefore have the feature vector for each day , and the closing price for each day\n",
        "\n",
        "Thence we can commence a supervised classification exercise\n",
        "\n",
        "\n",
        "**External Factors**\n",
        "\n",
        "There are factors beyond the market which may cause market movements ; these may be natural, political, or even cultural\n",
        "\n",
        "Poor monsoon might mean a downward trend for agricultural stocks ; a communist party in power might be bad news for financial firms :)\n",
        "\n",
        "If the relationship between external factors and the time series is significant then we need to consider them for our trading model to be robust\n",
        "\n",
        "Just a reminder for intuition : what do we classify ? That whether the instrument goes *up* or *down* ; not the value by which it moves"
      ],
      "metadata": {
        "id": "uvU49KKSYlzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Models\n",
        "\n",
        "**Logistic Regression**\n",
        "\n",
        "The logistic regression model provides the probability that a particular subsequent time period will be categorised as *up* or *down*\n",
        "\n",
        "We introduce the parameter of probability threshold ; if our probability comes up to be greater than the parameter, the financial instrument will move upwards ; and downwards for the other case\n",
        "\n",
        "In general we take the threshold to be 50%\n",
        "\n",
        "In essence, this is based on the logarithmic formula to model the probability of a positive based on continuous factors such as the lagged returns\n",
        "\n",
        "Let us consider the situation where we are interested in preducting the subsequent time period from the previous two lagged returns\n",
        "\n",
        "Denote them as L<sub>1</sub> and L<sub>2</sub>\n",
        "\n",
        "Then the probability of an upward movement can be denoted as :\n",
        "\n",
        "exp(b<sub>0</sub> + b<sub>1</sub> x L<sub>1</sub> + b<sub>2</sub> x L<sub>2</sub> )/( exp(b<sub>0</sub> + b<sub>1</sub> x L<sub>1</sub> + b<sub>2</sub> x L<sub>2</sub> ) + 1 )\n",
        "\n",
        "\n",
        "We use this instead of a linear regression because it provides a probability between 0 and 1 for all values of L<sub>1</sub> and L<sub>2</sub>\n",
        "\n",
        "To obtain the appropriate b<sub>i</sub> coefficients, the maximum likelihood method is used\n",
        "\n",
        "This is, fortunately, directly handled by the Scikit-Learn Library\n",
        "\n",
        "\n",
        "**Linear Discriminant Analysis**\n",
        "\n",
        "In logistic regression we modelled the probability of a true classification given the previous two lagged returns\n",
        "\n",
        "In LDA, the distribution of L<sub>i</sub> variables is modelled separately ; given the state of the system (either positive or negative)\n",
        "\n",
        "We assume that the predictors are drawn from a multivariate Gaussian distribution ; after calculating estimates for the parameters of the distribution , the parameters are inserted into Bayes' Theorem to make predictions on which class an observation belongs to\n",
        "\n",
        "Essentially it finds a linear combination of features that best separates two or more classes\n",
        "\n",
        "By projecting data onto a new axis that maximizes class separation, LDA improves class predictability while preserving class discriminatory information\n",
        "\n",
        "All classes share the same covariance matrix\n",
        "\n",
        "Again, the Scikit-Learn library handles this for us ; we do not need to code up the nitty-gritty of this\n",
        "\n",
        "**Quadratic Discriminant Analysis**\n",
        "\n",
        "Similar to LDA, except for the fact that separate classes have separate covariance matrices\n",
        "\n",
        "QDA performs better when *decision boundaries are non-linear*\n",
        "\n",
        "LDA performs better when there are fewer training observations and therefore reduction of the variance is a key concern\n",
        "\n",
        "QDA, on the other hand, performs better when there are a large number of training obserations and variance reduction would not change much\n",
        "\n",
        "**Support Vector Machines**\n",
        "\n",
        "In SVMs , we attempt to locate a linear separation boundary in the feature space that correctly classifies most, but of course not all, of the training observations by creating an optimal separation boundary between the two classes\n",
        "\n",
        "We can extend that capability to allow detection of non-linear decision boundaries\n",
        "\n",
        "This allows the enlargening of the feature space to include significant non-linearity\n",
        "\n",
        "SVMs allow non-linear decision boundaries via many different choices of kernel ; we are free to use kernels beyond linear systems, we may use quadratic or higher order polynomials, or even radial kernels to describe non-linear boundaries\n",
        "\n",
        "**Decision Trees**\n",
        "\n",
        "Decision trees are a supervised classification technique that utilise a tree structure to partition\n",
        "the feature space into recursive subsets via a \"decision\" at each node of the tree\n",
        "\n",
        "Let us consider an example ; suppose the question at the decision node is whether the price of a particular instrument was above or below a certain threshold ; this will divide the feature space into two subsets ; we can further ask the question if the volume was above or below a certain threshold , thus now creating 4 subsets\n",
        "\n",
        "This is a more intuitive and naturally interpretable classification mechanism as compared to SVMs or Discriminant Analysers\n",
        "\n",
        "**Random Forests**\n",
        "\n",
        "This is the domain where we start getting into ensemble learning\n",
        "\n",
        "Instead of attacking the problem with a single classifier, we create a large number of classifiers and train them all with varying parameters\n",
        "\n",
        "Then combine the results of the prediction in a weighted average to obtain a prediction accuracy that is greater than that brought on by any of the individual constituents\n",
        "\n",
        "This is the Random Forest ; which takes in multiple decision trees and combines the predictions\n",
        "\n",
        "**Principal Component Analysis**\n",
        "\n",
        "All of the above techniques belong to the *supervised classification* domain\n",
        "\n",
        "Alternatively, we can approach the problem not in terms of supervising the training procedure but instead allowing an algorithm to ascertain the relevant features on its own ; such methods are known as unsupervised learning algorithms\n",
        "\n",
        "The idea remains to reduce the number of dimensions of a problem to the relevant and statistically significant ones and ultimately discovering features that provide predictive power in the time series analysis\n",
        "\n",
        "One of the techniques to do this is Principal Component Analysis (PCA)\n",
        "\n",
        "The basic idea of a PCA is to transform a set of possibly correlated variables (such as with\n",
        "time series autocorrelation) into a set of linearly uncorrelated variables known as the principal\n",
        "components. Such principal components are ordered according to the amount of variance they\n",
        "describe, in an orthogonal manner. Thus if we have a very high-dimensional feature space (10+\n",
        "features), then we could reduce the feature space via PCA to perhaps 2 or 3 principal components\n",
        "that provide nearly all of the variability in the data, thus leading to a more robust supervised\n",
        "classifier model when used on this reduced dataset.\n",
        "\n",
        "\n",
        "\n",
        "Now we move on to coding up some stuff\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k5t2I3-sMev0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "\n",
        "\n",
        "def create_lagged_series(symbol, start_date, end_date, lags=5):\n",
        "    # Download stock information from Yahoo Finance\n",
        "    ts = yf.download(symbol, start=start_date - datetime.timedelta(days=365), end=end_date)\n",
        "\n",
        "    # Create the new lagged DataFrame\n",
        "    tslag = pd.DataFrame(index=ts.index)\n",
        "    tslag[\"Today\"] = ts[\"Adj Close\"]\n",
        "    tslag[\"Volume\"] = ts[\"Volume\"]\n",
        "\n",
        "    # Create the shifted lag series of prior trading period close values\n",
        "    # This has purely been done with the help of ChatGPT\n",
        "    for i in range(0, lags):\n",
        "        tslag[\"Lag%s\" % str(i+1)] = ts[\"Adj Close\"].shift(i+1)\n",
        "\n",
        "    # Create the returns DataFrame\n",
        "    tsret = pd.DataFrame(index=tslag.index)\n",
        "    tsret[\"Volume\"] = tslag[\"Volume\"]\n",
        "    tsret[\"Today\"] = tslag[\"Today\"].pct_change() * 100.0\n",
        "\n",
        "    # If any of the values of percentage returns equal zero, set them to\n",
        "    # a small number (this helps in dealing with issues in the QDA model in scikit-learn)\n",
        "    tsret[\"Today\"].replace(0, 0.0001, inplace=True)\n",
        "\n",
        "    # Create the lagged percentage returns columns\n",
        "    for i in range(0, lags):\n",
        "        tsret[\"Lag%s\" % str(i+1)] = tslag[\"Lag%s\" % str(i+1)].pct_change() * 100.0\n",
        "\n",
        "    # Create the \"Direction\" column (+1 or -1) indicating an up/down day\n",
        "    tsret[\"Direction\"] = np.sign(tsret[\"Today\"])\n",
        "    tsret = tsret[tsret.index >= start_date]\n",
        "\n",
        "    # Drop rows with NaN values\n",
        "    tsret.dropna(inplace=True)\n",
        "\n",
        "    return tsret\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # I like dealing with the AMZN stock\n",
        "    snpret = create_lagged_series(\"AMZN\", datetime.datetime(2020, 1, 10), datetime.datetime(2023, 12, 31), lags=5)\n",
        "\n",
        "    # Here we use the prior two days of returns as predictor\n",
        "    X = snpret[[\"Lag1\", \"Lag2\"]]\n",
        "    y = snpret[\"Direction\"]\n",
        "\n",
        "    # The test data is split into two parts: Before and after 1st Jan 2023.\n",
        "    start_test = datetime.datetime(2023, 1, 1)\n",
        "\n",
        "    X_train = X[X.index < start_test]\n",
        "    X_test = X[X.index >= start_test]\n",
        "    y_train = y[y.index < start_test]\n",
        "    y_test = y[y.index >= start_test]\n",
        "\n",
        "    print(\"Hit Rates/Confusion Matrices:\\n\")\n",
        "    models = [\n",
        "        (\"LR\", LogisticRegression(max_iter=1000)),\n",
        "        (\"LDA\", LDA()),\n",
        "        (\"QDA\", QuadraticDiscriminantAnalysis()),\n",
        "        (\"LSVC\", LinearSVC(max_iter=10000)),\n",
        "        (\"RSVM\", SVC(\n",
        "            C=1000000.0, cache_size=200, class_weight=None,\n",
        "            coef0=0.0, degree=3, gamma=0.0001, kernel='rbf',\n",
        "            max_iter=-1, probability=False, random_state=None,\n",
        "            shrinking=True, tol=0.001, verbose=False)\n",
        "        ),\n",
        "        (\"RF\", RandomForestClassifier(\n",
        "            n_estimators=1000, criterion='gini',\n",
        "            max_depth=None, min_samples_split=2,\n",
        "            min_samples_leaf=1, max_features='auto',\n",
        "            bootstrap=True, oob_score=False, n_jobs=1,\n",
        "            random_state=None, verbose=0)\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    #I really do not understand the multiply hyperparameters of RSVM and RF ; need to understand that\n",
        "\n",
        "\n",
        "    for m in models:\n",
        "        m[1].fit(X_train, y_train)\n",
        "        pred = m[1].predict(X_test)\n",
        "        print(\"%s:\\n%0.3f\" % (m[0], m[1].score(X_test, y_test)))\n",
        "        print(\"%s\\n\" % confusion_matrix(y_test, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3AN_y43fZwb",
        "outputId": "db641e09-e264-4fd4-e5ca-e254d0274db2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit Rates/Confusion Matrices:\n",
            "\n",
            "LR:\n",
            "0.540\n",
            "[[ 14  98]\n",
            " [ 17 121]]\n",
            "\n",
            "LDA:\n",
            "0.540\n",
            "[[ 14  98]\n",
            " [ 17 121]]\n",
            "\n",
            "QDA:\n",
            "0.452\n",
            "[[ 76  36]\n",
            " [101  37]]\n",
            "\n",
            "LSVC:\n",
            "0.540\n",
            "[[ 14  98]\n",
            " [ 17 121]]\n",
            "\n",
            "RSVM:\n",
            "0.548\n",
            "[[  8 104]\n",
            " [  9 129]]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF:\n",
            "0.460\n",
            "[[41 71]\n",
            " [64 74]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "Almost all of the hit rates lie between 45% and 55%\n",
        "\n",
        "It is clear that lagged time series analysis alone is not a good enough forecasting predictor and we need to improve\n",
        "\n",
        "The true negative rates in general are much higher than true positives (except for QDA) : as such , this makes for a much better shorting strategy as compared to a long strategy"
      ],
      "metadata": {
        "id": "zwELo9who_Yt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJEnLu-zWpuj"
      },
      "outputs": [],
      "source": []
    }
  ]
}