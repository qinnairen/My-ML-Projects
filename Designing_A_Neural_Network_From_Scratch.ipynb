{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Yk_H2IauJOwD"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are attempting to create a Neural Network for a Random Number Generator by scartch using only Numpy and Panda\n",
        "\n",
        "We define these two very popular activation functions which can otherwise be found in the PyTorch library\n",
        "\n",
        "These are both differentiable in  general ; ReLU is indifferentiable near 0 though"
      ],
      "metadata": {
        "id": "1XXVpz0NJZMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x, derivative=False):\n",
        "    if derivative:\n",
        "        return 1.0 - tanh(x)**2\n",
        "    else:\n",
        "        return np.tanh(x)\n",
        "\n",
        "def ReLU(x, derivative=False):\n",
        "    if derivative:\n",
        "        return 1 * (x > 0)  #returns 1 for any x > 0, and 0 otherwise\n",
        "\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def mse(target, actual, derivative=False):\n",
        "    try:\n",
        "        assert(target.shape == actual.shape)\n",
        "    except AssertionError:\n",
        "        print(f\"Shape of target vector: {target.shape} does not match shape of actual vector: {actual.shape}\")\n",
        "        raise\n",
        "\n",
        "    if derivative:\n",
        "        error = (actual - target)\n",
        "\n",
        "    else:\n",
        "        error = np.sum(0.5 * np.sum((target-actual)**2, axis=1, keepdims=True))\n",
        "\n",
        "    return error"
      ],
      "metadata": {
        "id": "s_3QpU1BJdRC"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us try to understand what particular terms mean ; the weight and bias matrices are obvious\n",
        "\n",
        "Topology refers to the structure of the neural network ; if topology is a vector as [2,6,4,7] ; then we are essentially being told that the neural network has 4 layers : the first one with 2 neurons , the second with 6 neurons , the third with 4 neurons, and the fourth with 7 neuronns\n",
        "\n",
        "Momentum is a term used for optimization purposes : without Momentum the weights might oscillate and take longer to converge whereas with Momentum the updates are smoother, and the weights converge faster because the momentum term helps to maintain a direction and dampens oscillations\n",
        "\n",
        "It will be really helpful if you ask ChatGPT to give you an illustraton of how momentum works for a quadratic loss function\n",
        "\n",
        "To really understand how netIns and netOuts work, let us consider a working example of what is really going on behind the scenes in the neural network\n",
        "\n",
        "Suppose we have the topology of a neural network as [2,3,1] ; this implies we have three layers: an input layer, a hidden layer, and an output layer ; the weights matrix from the input layer to the hidden layer will be a 2x3 matrix while the weights matrix from the hidden layer to the output layer will be a 3x1 matrix\n",
        "\n",
        "The bias vector will of course be a 3x1 vector for the first pass and a 1x1 vector for the second pass\n",
        "\n",
        "Suppose the input is some x ; then netIn will be a 3x1 vector which will be the result of application of the weights and biases from the input layer to the hidden layer\n",
        "\n",
        "When we apply the appropriate activation function on the netIn , we get the netOut\n",
        "\n",
        "This netOut then is sent forward to the next layer as input and the process continues\n",
        "\n",
        "\n",
        "The initialization of weights and biases method is just to randomly generate matrices and vectors of appropriate size required as per the topology of the neural network\n",
        "\n",
        "As for the Xavier Initialization : The primary goal of Xavier initialization is to keep the scale of the gradients roughly the same in all layers of the network. This helps to mitigate the problem of vanishing or exploding gradients, which can occur when training deep neural networks.\n",
        "\n",
        "So what we have with Xavier initialization simply is a uniform distribution between negative sqrt of 6/(inputDimensions + outputDimensions)\n",
        "\n",
        "However that formula can be varied as per requirements\n",
        "\n",
        "The feedforward step is pretty straightforward ; we take a dot product between the input and the weight matrix for that layer , add it to the bias vector and proceed to fetcht the ouput by applying the activation function on that vector\n",
        "\n",
        "And we return that output\n",
        "\n",
        "\n",
        "Moving on to the gradient descent section, we have, the following parameters :\n",
        "layer_idx is the index of the layer whose weights and biases are being updated ;\n",
        "gradient_mat is the gradient of the loss with respect to the weights of the current layer ; bias_gradient is the gradient of the loss with respect to the biases of the current layer\n",
        "\n",
        "The term (self.momentum * self.last_change[layer_idx]) helps smooth the updates by incorporating a fraction of the previous weight change, which helps in reducing oscillations and speeds up convergence\n",
        "\n",
        "The term -(self.learning_rate * gradient_mat[layer_idx]) is the standard gradient descent step, where the learning rate controls the size of the step in the direction of the negative gradient\n",
        "\n",
        "The same calculation is applied to the biases using self.last_bias_change[layer_idx] and bias_gradient\n",
        "\n",
        "In backpropagation , we begin from the output layer, which is the last layer, to work back on updating the weights and biases so as to optimize our neural network to give better results\n",
        "\n",
        "We calculate the derivative of the activation function, then the derivative of the error function , and multiply those two terms in order to get the delta (change) we wish to make to our parameters\n",
        "\n",
        "This process is then carried forward in the training function, which takes place for a number of epochs , each epoch further improving (hopefully) our model and seeking to minimize the loss\n",
        "\n",
        "This is what really takes place behind the scenes of a neural network"
      ],
      "metadata": {
        "id": "7eead_0KLBMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(object):\n",
        "    randomNumberGenerator = np.random.default_rng()\n",
        "\n",
        "    def __init__(self,\n",
        "                 topology:list[int] = [],\n",
        "                 learning_rate = 0.01,\n",
        "                 momentum      = 0.1,\n",
        "                 hidden_activation_func=ReLU,\n",
        "                 output_activation_func=tanh,\n",
        "                 init_method='random'):\n",
        "\n",
        "        self.topology    = topology\n",
        "        self.weight_mats = []\n",
        "        self.bias_mats   = []\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum      = momentum\n",
        "\n",
        "        self.hidden_activation = hidden_activation_func\n",
        "        self.output_activation = output_activation_func\n",
        "\n",
        "\n",
        "        self._init_weights_and_biases(init_method)\n",
        "        self.size             = len(self.weight_mats)\n",
        "        self.netIns           = [None] * self.size\n",
        "        self.netOuts          = [None] * self.size\n",
        "\n",
        "        self.last_change = [np.zeros(mat.shape) for mat in self.weight_mats]\n",
        "        self.last_bias_change      = [np.zeros(mat.shape) for mat in self.bias_mats]\n",
        "\n",
        "    def _init_weights_and_biases(self, method='random'):\n",
        "        if method.lower() == 'random':\n",
        "            _init_func = lambda num_rows, num_cols: self.randomNumberGenerator.random(size=(num_rows, num_cols))\n",
        "\n",
        "        elif method.lower() == 'xavier':\n",
        "            _init_func = self._xavier_weight_initialization\n",
        "\n",
        "        else:\n",
        "            print(f\"\\t-> initialization method {method} not recognized. Defaulting to 'random'\")\n",
        "            _init_func = lambda num_rows, num_cols: self.randomNumberGenerator.random(size=(num_rows, num_cols))\n",
        "\n",
        "        #-- set up matrices\n",
        "        if len(self.topology) > 1:\n",
        "            j = 1\n",
        "            for i in range(len(self.topology)-1):\n",
        "                num_rows = self.topology[i]\n",
        "                num_cols = self.topology[j]\n",
        "\n",
        "                mat         = _init_func(num_rows, num_cols)  #the +1 accounts for the bias weights\n",
        "                bias_vector = _init_func(1, num_cols)\n",
        "\n",
        "                self.weight_mats.append(mat)\n",
        "                self.bias_mats.append(bias_vector)\n",
        "\n",
        "                j += 1\n",
        "\n",
        "\n",
        "    def _xavier_weight_initialization(self, num_rows, num_cols):\n",
        "        '''A type of weight initialization that seems to be tailored to sigmoidal activation functions.\n",
        "        Here is a reference: https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/'''\n",
        "        num_inputs = self.topology[0]\n",
        "\n",
        "        lower_bound = -1 / np.sqrt(num_inputs)\n",
        "        upper_bound = 1 / np.sqrt(num_inputs)\n",
        "\n",
        "        mat = self.randomNumberGenerator.uniform(lower_bound, upper_bound, (num_rows, num_cols))\n",
        "        return mat\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return tuple(self.topology)\n",
        "\n",
        "    @property\n",
        "    def n_trainable_params(self):\n",
        "        n_params = 0\n",
        "        for weight_mat, bias_mat in zip(self.weight_mats, self.bias_mats):\n",
        "            n_params += weight_mat.size + bias_mat.size\n",
        "\n",
        "        return n_params\n",
        "\n",
        "\n",
        "\n",
        "    def feedforward(self, input_vector):\n",
        "\n",
        "        self.netIns.clear()\n",
        "        self.netOuts.clear()\n",
        "\n",
        "        I = input_vector\n",
        "\n",
        "        for idx, W in enumerate(self.weight_mats):\n",
        "\n",
        "            bias_vector = self.bias_mats[idx]\n",
        "\n",
        "            self.netOuts.append(I)\n",
        "            I = np.dot(I, W) + bias_vector\n",
        "            self.netIns.append(I)\n",
        "\n",
        "            #-- apply activation function\n",
        "            if idx == len(self.weight_mats) - 1:\n",
        "                out_vector = self.output_activation(I)\n",
        "            else:\n",
        "                I          = self.hidden_activation(I)\n",
        "\n",
        "        return out_vector\n",
        "\n",
        "\n",
        "    def _gradient_descent(self, layer_idx, gradient_mat, bias_gradient):\n",
        "\n",
        "        delta_weight = (self.momentum * self.last_change[layer_idx]) - (self.learning_rate * gradient_mat[layer_idx])\n",
        "\n",
        "        delta_bias_weights =  (self.momentum * self.last_bias_change[layer_idx]) \\\n",
        "                            - (self.learning_rate * bias_gradient)\n",
        "\n",
        "        self.weight_mats[layer_idx] += delta_weight\n",
        "        self.bias_mats[layer_idx]   += delta_bias_weights\n",
        "\n",
        "        self.last_change[layer_idx]      = 1 * delta_weight\n",
        "        self.last_bias_change[layer_idx] = 1 * delta_bias_weights\n",
        "\n",
        "    def backprop(self,\n",
        "                 target,\n",
        "                 output,\n",
        "                 error_func,):\n",
        "\n",
        "        for i in range(self.size):\n",
        "            back_index =self.size-1 -i\n",
        "\n",
        "            if i == 0:\n",
        "                d_activ = self.output_activation(self.netIns[back_index], derivative=True)\n",
        "                d_error = error_func(target,output,derivative=True)\n",
        "                delta = d_error * d_activ\n",
        "\n",
        "\n",
        "                gradient_mat  = np.dot(self.netOuts[back_index].T , delta)\n",
        "                bias_grad_mat = 1 * delta\n",
        "\n",
        "                self._gradient_descent(layer_idx=back_index, gradient_mat=gradient_mat, bias_gradient=bias_grad_mat)\n",
        "\n",
        "            else:\n",
        "                W_trans = self.weight_mats[back_index+1].T\n",
        "                d_activ = self.hidden_activation(self.netIns[back_index],derivative=True)\n",
        "                d_error = np.dot(delta, W_trans)\n",
        "                delta = d_error * d_activ\n",
        "\n",
        "                gradient_mat = np.dot(self.netOuts[back_index].T , delta)\n",
        "                bias_grad_mat = 1 * delta\n",
        "\n",
        "                self._gradient_descent(layer_idx=back_index, gradient_mat=gradient_mat, bias_gradient=bias_grad_mat)\n",
        "\n",
        "\n",
        "    def train(self, input_set, target_set, epochs=1000, batch_size=0, error_threshold=1E-10, error_func=mse, verbose=True):\n",
        "\n",
        "        if batch_size == 0:\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                error = 0\n",
        "\n",
        "                for i in range(len(input_set)):\n",
        "                    inputs = input_set[i:i+1]\n",
        "                    targets = target_set[i:i+1]\n",
        "\n",
        "                    error += self._train_helper(inputs, targets, error_func)\n",
        "\n",
        "                if verbose and (epoch % 20 == 0):\n",
        "                    self._print_training_info(epoch, epochs, error, error_threshold)\n",
        "\n",
        "                if error <= error_threshold:\n",
        "                    print(f\"\\t-> error {error} is lower than threshold {error_threshold}\\n\\tStopped at epoch {epoch}\")\n",
        "                    break\n",
        "\n",
        "        elif batch_size == -1:\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                error = 0\n",
        "\n",
        "                inputs  = input_set\n",
        "                targets = target_set\n",
        "\n",
        "                error += self._train_helper(inputs, targets, error_func)\n",
        "\n",
        "\n",
        "                if verbose and (epoch % 20 == 0):\n",
        "                    self._print_training_info(epoch, epochs, error, error_threshold)\n",
        "\n",
        "                if error <= error_threshold:\n",
        "                        print(f\"\\t-> error {error} is lower than threshold {error_threshold}\\n\\tStopped at epoch {epoch}\")\n",
        "                        break\n",
        "\n",
        "        else:\n",
        "            print(\"\\t-> PROBLEM: mini-batches not supported yet. Choose batch_size 0 or -1\")\n",
        "\n",
        "        return error\n",
        "\n",
        "    def _print_training_info(self, curr_epoch, total_epochs, curr_error, error_threshold):\n",
        "        text = f\"\"\"{'-'*45}\\n\\t-> training step: :{curr_epoch}/{total_epochs}\\n\\t\\t* current error: {curr_error}, threshold: {error_threshold}\\n\"\"\"\n",
        "        print(text)\n",
        "\n",
        "    def _train_helper(self, input_set, target_set, error_func):\n",
        "        nnet_output = self.feedforward(input_set)\n",
        "        error       = error_func(target_set, nnet_output)\n",
        "\n",
        "        self.backprop(target=target_set, output=nnet_output, error_func=error_func,)\n",
        "        return error"
      ],
      "metadata": {
        "id": "QIWlaRVXJfxX"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we begin designing our logic gates\n",
        "\n",
        "We have four gates here and we train on the NAND gates target\n",
        "\n",
        "We can similarly train on the targets of other gates and get all the gates"
      ],
      "metadata": {
        "id": "1pzJHOh3drgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "inputs  = np.array([[0,0],\n",
        "                    [0,1],\n",
        "                    [1,0],\n",
        "                    [1,1]])\n",
        "\n",
        "#outputs of different logic gates when given the inputs above\n",
        "training_targets = {\n",
        "    'XOR'  : np.array([[0],\n",
        "                       [1],\n",
        "                       [1],\n",
        "                       [0]]),\n",
        "\n",
        "    'OR'   : np.array([[0],\n",
        "                       [1],\n",
        "                       [1],\n",
        "                       [1]]),\n",
        "\n",
        "    'AND'  : np.array([[0],\n",
        "                       [0],\n",
        "                       [0],\n",
        "                       [1]]),\n",
        "\n",
        "    'NAND' : np.array([[1],\n",
        "                       [1],\n",
        "                       [1],\n",
        "                       [0]]),\n",
        "}"
      ],
      "metadata": {
        "id": "UjyCpJEYJibe"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a neural network for the XOR logic gate, so two inputs and one output neuron.\n",
        "nnet = NeuralNetwork([2, 3, 1], hidden_activation_func=ReLU, output_activation_func=tanh, init_method='random')\n",
        "\n",
        "nnet.train(inputs, training_targets['NAND'], epochs=1000, batch_size=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey2vVH_SJka0",
        "outputId": "dd4ad33d-9e7d-4672-a3a1-bba2d629b81e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------\n",
            "\t-> training step: :0/1000\n",
            "\t\t* current error: 0.5008516419361585, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :20/1000\n",
            "\t\t* current error: 0.5008190858226067, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :40/1000\n",
            "\t\t* current error: 0.5007875130369377, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :60/1000\n",
            "\t\t* current error: 0.5007567438525375, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :80/1000\n",
            "\t\t* current error: 0.5007267329189224, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :100/1000\n",
            "\t\t* current error: 0.5006974376065778, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :120/1000\n",
            "\t\t* current error: 0.5006688177886365, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :140/1000\n",
            "\t\t* current error: 0.50064083564235, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :160/1000\n",
            "\t\t* current error: 0.5006134554682561, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :180/1000\n",
            "\t\t* current error: 0.5005866435251969, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :200/1000\n",
            "\t\t* current error: 0.5005603678795513, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :220/1000\n",
            "\t\t* current error: 0.5005345982672351, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :240/1000\n",
            "\t\t* current error: 0.5005093059671869, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :260/1000\n",
            "\t\t* current error: 0.5004844636851962, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :280/1000\n",
            "\t\t* current error: 0.5004600454470594, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :300/1000\n",
            "\t\t* current error: 0.5004360265001562, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :320/1000\n",
            "\t\t* current error: 0.5004123832226359, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :340/1000\n",
            "\t\t* current error: 0.5003890930394898, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :360/1000\n",
            "\t\t* current error: 0.5003661343448551, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :380/1000\n",
            "\t\t* current error: 0.5003434864299703, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :400/1000\n",
            "\t\t* current error: 0.5003211294162523, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :420/1000\n",
            "\t\t* current error: 0.5002990441930216, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :440/1000\n",
            "\t\t* current error: 0.5002772123594474, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :460/1000\n",
            "\t\t* current error: 0.5002556161703232, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :480/1000\n",
            "\t\t* current error: 0.5002342384853212, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :500/1000\n",
            "\t\t* current error: 0.5002130627214072, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :520/1000\n",
            "\t\t* current error: 0.5001920728081217, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :540/1000\n",
            "\t\t* current error: 0.5001712531454647, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :560/1000\n",
            "\t\t* current error: 0.500150588564137, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :580/1000\n",
            "\t\t* current error: 0.5001300642879205, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :600/1000\n",
            "\t\t* current error: 0.5001096658979871, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :620/1000\n",
            "\t\t* current error: 0.5000893792989506, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :640/1000\n",
            "\t\t* current error: 0.5000691906864863, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :660/1000\n",
            "\t\t* current error: 0.5000490865163552, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :680/1000\n",
            "\t\t* current error: 0.5000290534746843, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :700/1000\n",
            "\t\t* current error: 0.5000090784493592, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :720/1000\n",
            "\t\t* current error: 0.49998914850239967, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :740/1000\n",
            "\t\t* current error: 0.499969250843193, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :760/1000\n",
            "\t\t* current error: 0.49994937280246593, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :780/1000\n",
            "\t\t* current error: 0.49992950180688583, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :800/1000\n",
            "\t\t* current error: 0.49990962535418243, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :820/1000\n",
            "\t\t* current error: 0.4998897309886882, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :840/1000\n",
            "\t\t* current error: 0.4998698062771976, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :860/1000\n",
            "\t\t* current error: 0.499849838785049, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :880/1000\n",
            "\t\t* current error: 0.4998298160523347, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :900/1000\n",
            "\t\t* current error: 0.4998097255701458, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :920/1000\n",
            "\t\t* current error: 0.49978955475676046, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :940/1000\n",
            "\t\t* current error: 0.4997692909336824, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :960/1000\n",
            "\t\t* current error: 0.4997489213014381, threshold: 1e-10\n",
            "\n",
            "---------------------------------------------\n",
            "\t-> training step: :980/1000\n",
            "\t\t* current error: 0.4997284329150398, threshold: 1e-10\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4997088470083615"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    }
  ]
}